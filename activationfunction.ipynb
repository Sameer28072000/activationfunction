{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f34120d-0392-4371-8041-b80d4a6b8c6e",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1f15d-b9e9-4849-8a84-38720e193706",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-An activation function is a mathematical function that introduces non-linearity into the network.\n",
    "      The purpose of an activation function is to introduce non-linearities into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba34390-5bfd-4862-b28f-d279e6d5585c",
   "metadata": {},
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc5308-8b79-45a8-8dfa-0879602ac4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-The common types of activation functions used in neural networks:\n",
    "      i.Sigmoid function\n",
    "        ii.ReLU (Rectified Linear Unit)\n",
    "          iii.Leaky ReLU\n",
    "            iv.softmax\n",
    "              v.Hyperbolic Tangent(tanh)\n",
    "                vi.Parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f823f-171c-43ec-8c36-cf5b8bdf36a0",
   "metadata": {},
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5ab8d-77d4-475a-82d1-d63684394798",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Activation functions play a crucial role in the training process and performance of a neural network.\n",
    "        Some ways of activation function affect the traing process:\n",
    "          i.Non-linearity\n",
    "            ii.Gradient flow\n",
    "              iii.Activation range\n",
    "                iv.Sparse activation\n",
    "                  v.Computation efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e4002-4ed6-41e1-8992-d81550c568d1",
   "metadata": {},
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a65be-f58c-40ed-b6d3-d6c9e120044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-The sigmoid activation function, also known as the logistic function, \n",
    "        is a commonly used activation function in neural networks.\n",
    "    \n",
    "    sigma(x)=1/(1+e^(-x))\n",
    "    \n",
    "    Advantages of Sigmoid Function:-\n",
    "      i.Output range\n",
    "        ii.Differentiability\n",
    "        \n",
    "        Disadvantages of sigmoid function:-\n",
    "          i.Vanishing gradient\n",
    "            ii.Output saturation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc283d-5f3d-471d-a506-18b22a70a5d3",
   "metadata": {},
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53134a-60c8-4eec-b85b-18846c519a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-The rectified linear unit (ReLU) activation function is a popular activation function used in neural networks.\n",
    "      ReLU=max(0,x)\n",
    "    \n",
    "      Differences between ReLU and the sigmoid function:\n",
    "        i.Non-linearity\n",
    "          ii.Sparsity\n",
    "            iii.Saturation\n",
    "              iv.Output range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57d7b0-3fd3-46e1-9e01-f2b4431e96e2",
   "metadata": {},
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c012efe-ea80-4cb5-9c09-b95715264be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Benefits of using the ReLU activation function over the sigmoid function:\n",
    "      i.Avoiding vanishing gradient\n",
    "        ii.improved training speed\n",
    "          iii.Sparse activation\n",
    "            iv.Computation efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0527a2f-60d0-4b51-969a-c5b13cc9dbe2",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94639a-ca3f-4eb2-bd50-b8f7ae4ace21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Leaky ReLU is a variation of the rectified linear unit (ReLU) activation function.\n",
    "  The small positive slope in leaky ReLU ensures that there is always a gradient present, even for negative inputs.\n",
    "     It helps to vanishing gradient prblm & allows for more stable & consistent gradient flow during backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04564df-3639-4f38-8ecb-67c132eb5bca",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc91f2-94cb-4be3-829b-1f4e12854722",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "     The softmax activation function is commonly used in the output layer of neural networks, \n",
    "            particularly in multi-class classification problems.\n",
    "         The softmax activation function is commonly used when the goal is to assign an input sample\n",
    "              to one of multiple exclusive classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3f947-5f4a-44ae-893c-8fe502bb02ee",
   "metadata": {},
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9116474-9503-4c6c-a8da-250a77624f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "     The hyperbolic tangent (tanh) activation function is a commonly used activation function in neural networks.\n",
    "    \n",
    "    tanh=(e^x-e^(-x))/e^x+e^(-x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
